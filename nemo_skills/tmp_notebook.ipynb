{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('/home/jovyan/medvedev/NeMo-Skills/nemo_skills/augmented_problems2.jsonl', lines=True)\n",
    "df_orginal_problems = pd.read_json('/home/jovyan/medvedev/foundation_model/foundation_model/data/preprocessed/train/planner_train_small_onemessage.jsonl',  lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['original_problem', 'augmented_problem'], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'instruction', 'generation', 'response', 'feedback',\n",
       "       'score', 'model_output', 'task', 'conditions', 'problem'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orginal_problems.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sentence transformer model\n",
    "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "\n",
    "# Step 1: Merge original and augmented datasets\n",
    "# Assume `original_id` is a unique identifier for each original problem if available\n",
    "merged_df = pd.merge(df, df_orginal_problems, left_on='original_problem', right_on='problem', how='inner')\n",
    "\n",
    "# Step 2: Concatenate 'problem' and 'augmented_problem' columns into a single column for deduplication\n",
    "merged_df = pd.concat(\n",
    "    [merged_df[['instruction', 'generation', 'response', 'feedback', 'score', 'model_output', 'task', 'conditions', 'problem']],\n",
    "    merged_df[['augmented_problem']].rename(columns={'augmented_problem': 'problem'})],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# Step 3: Deduplicate based on exact matches\n",
    "dedup_df = merged_df.drop_duplicates(subset=['problem']).reset_index(drop=True)\n",
    "\n",
    "# Step 4: Deduplicate based on cosine similarity\n",
    "# Compute embeddings for cosine similarity deduplication\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecfa11a56b9640f493dc713acfe04163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/774 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings = model.encode(dedup_df['problem'].tolist(), convert_to_tensor=True, show_progress_bar=True, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "duplicated_indexes = set()\n",
    "first_occurrences = set()\n",
    "similarity_threshold = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_duplicates_cosine(embeddings: torch.Tensor, batch_size: int = 128, similarity_threshold: float = 0.91):\n",
    "    duplicated_indexes = set()\n",
    "    first_occurrences = set()\n",
    "    \n",
    "    for i in tqdm(range(0, embeddings.size(0), batch_size)):\n",
    "        # Get batch embeddings for comparisons\n",
    "        batch_embeddings = embeddings[i:i + batch_size]\n",
    "\n",
    "        # Calculate cosine similarities between batch and all embeddings\n",
    "        cosine_scores = util.cos_sim(batch_embeddings, embeddings)\n",
    "\n",
    "        # Identify duplicates in each batch\n",
    "        for batch_idx in range(batch_embeddings.size(0)):\n",
    "            global_idx = i + batch_idx  # Get global index for the current row\n",
    "\n",
    "            # Skip if the current index is already marked as a duplicate\n",
    "            if global_idx in duplicated_indexes:\n",
    "                continue\n",
    "\n",
    "            # Find top similarity scores for current row in batch\n",
    "            topk = torch.topk(cosine_scores[batch_idx], k=2)\n",
    "\n",
    "            # Check if the highest similarity (excluding self) is above the threshold\n",
    "            if topk.values[1].item() >= similarity_threshold:\n",
    "                # Get the index of the most similar row\n",
    "                duplicate_idx = topk.indices[1].item()\n",
    "\n",
    "                # Keep the first occurrence and mark subsequent duplicates\n",
    "                if duplicate_idx in first_occurrences:\n",
    "                    duplicated_indexes.add(global_idx)\n",
    "                else:\n",
    "                    first_occurrences.add(global_idx)\n",
    "                    \n",
    "    return duplicated_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6189/6189 [03:20<00:00, 30.81it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "228651"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicated_indexes = find_duplicates_cosine(embeddings)\n",
    "len(duplicated_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deduplicated = dedup_df.drop(index=list(duplicated_indexes)).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deduplicated.to_json(\"deduplicated_augumented_problems.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amedvedev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
